{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "352cbccb-93ae-415f-a55c-fd2ff8ba5652",
   "metadata": {},
   "source": [
    "# LinkedIn Job Search WebScraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833ef5a2-a74e-4edc-bfb6-9b36a6684f27",
   "metadata": {},
   "source": [
    "## Possible use case:\n",
    "\n",
    "This function can be helpful for finding a suitable job for a job seeker in the market that he/she is interested in in a more efficient way. \n",
    "\n",
    "Simply typing the keyword into the search bar is usually not very helpful if a person wants to have better matches for their career goals or the area they have expertise due to a lack of existing search options. Plus it is usually the case that one does not have the time to go through all of the job postings which means that they might not even get to the posting that suits them the best simply because they were not sponsored enough.\n",
    "\n",
    "With this simple function, one can generate a data frame of job listings for as many as they want to have and then search for phrase or keyword directly in the Job description.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e12b71b-7d21-4492-9186-31c2ff9af2d6",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "26efd628-0db8-4dd8-a7df-1a8f5a27537c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "from yaspin import yaspin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a079f4-0ecd-4113-8405-84a7a3a731d9",
   "metadata": {},
   "source": [
    "## Creating a function that would generate the url given the inputs.\n",
    "\n",
    "### Inputs\n",
    "__title:__ the search keywords ex: \"data scientist\" or \"senior data analyst\" <br>\n",
    "__location:__ location of the Job that we want to search for. ex: \"Hamburg\" or \"Germany\" <br>\n",
    "__start_num:__  since the search result is seperated to pages, this indicates from which Job posting number the result will be shown. We will iterate thorough these pages in the main function later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eb73cd51-aeab-4aa7-bcc3-e509ad699f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_url(title,location,start_num):\n",
    "\n",
    "    title_split = title.split()\n",
    "    seperator=\"%2B\"\n",
    "    search_title=seperator.join(title_split)\n",
    "    \n",
    "    return(\"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=\"+search_title+\"&location=\"+location+\"&start=\"+str(start_num))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6319a0cb-b269-4509-b8ac-693de334de91",
   "metadata": {},
   "source": [
    "## Creating functions that would extract the parts from the html that we are interested in.\n",
    "__Functions will extract:__ location, company name, job title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a887745c-91cd-4f76-b512-8f732d15f5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(soup):\n",
    "    job_list=[]\n",
    "\n",
    "    for item in soup.find_all(\"h3\"):\n",
    "        job_list.append(item.get_text(strip = True))\n",
    "\n",
    "    return(job_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b87199d5-4421-495e-9d7e-b58f8a1d395d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company(soup):\n",
    "    company_list=[]\n",
    "    \n",
    "    for item in soup.find_all(\"h4\"):\n",
    "        company_list.append(item.get_text(strip = True))\n",
    "    return(company_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d08c996b-d423-4ad9-bed9-489f13a75ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_location(soup):\n",
    "    location_list=[]\n",
    "    \n",
    "    for item in soup.find_all(\"span\", {\"class\": \"job-search-card__location\"}):\n",
    "        location_list.append(item.get_text(strip=True))\n",
    "    return(location_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5d86a1-8624-41a7-9194-ae360ebe1cb1",
   "metadata": {},
   "source": [
    "## get_job_id (function) \n",
    "This function fetches the Job ids one by one from the listing. As I have encountered postings with no id from time to time I have added the try except handling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "710198aa-d36c-42a1-854d-b24b7b0138d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_id (soup):\n",
    "#soup=soup_job_listing    \n",
    "    job_id_list=[]\n",
    "    for job in soup.find_all(\"li\"):\n",
    "        base_card=job.find(\"div\", {\"class\": \"base-card\"})\n",
    "        try:\n",
    "           job_id_list.append(base_card.get(\"data-entity-urn\").split(\":\")[3])\n",
    "        except:\n",
    "            job_id_list.append(\"NA\")\n",
    "   \n",
    "    return(job_id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95df07b3-d0ae-4103-a3ad-88a2f66371ff",
   "metadata": {},
   "source": [
    "## get_linkedin_job_listing\n",
    "The main function that brings everything together.\n",
    "\n",
    "### Inputs:\n",
    "__title:__ will be passed through to the url generating function that is created in the first step. <br>\n",
    "__location:__ will be passed through to the same url generating function.<br>\n",
    "__number:__ is the minimum number of job postings that needs to be extracted. Since the resulting pages only show 10 job postings at a time, if the number equals 25, it will extract 30 and if equals 81, it will extract the first 90 job postings. \n",
    "\n",
    "### Outer Loop\n",
    "First loop will iterate through all the pages to achieve the minimum number of job postings and collects the data into seperate lists.\n",
    "\n",
    "### Inner Loop\n",
    "Iterates through individual job listings on each search response page. It requests the individual job listing page and collects the job descriptions in a list.<br>\n",
    "Again try except handling was added as, although very rarely, some listins were missing id. \n",
    "\n",
    "### The Result\n",
    "The result is a data frame object with all the listings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4fd6fa00-33ed-4e01-aabd-0f9d16109835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linkedin_job_listing(title,location,number):\n",
    "    \n",
    "    with yaspin(text=\"Processing, please wait...\") as spinner:\n",
    "    \n",
    "        number_list=list(range(1,number,10))\n",
    "        job_list_all=[]\n",
    "        company_list_all=[]\n",
    "        location_list_all=[]\n",
    "        job_id_list_all=[]\n",
    "        job_desc_list_all=[]\n",
    "        url_list_all=[]\n",
    "    \n",
    "        for number in number_list:\n",
    "            job_search_url=generate_url(title,location,number)\n",
    "            r=requests.get(job_search_url)\n",
    "            soup_job_listing=bs(r.content, 'html.parser')\n",
    "        \n",
    "            job_list_all.extend(get_title(soup_job_listing))\n",
    "            company_list_all.extend(get_company(soup_job_listing))\n",
    "            location_list_all.extend(get_location(soup_job_listing))\n",
    "            job_id_list_all.extend(get_job_id(soup_job_listing))\n",
    "            job_id_list=get_job_id(soup_job_listing)\n",
    "#    print(r)\n",
    "\n",
    "    \n",
    "  #  job_description_list=[]\n",
    "   # url_list=[]\n",
    "            for job_id in job_id_list:\n",
    "                url=f\"https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/{job_id}\"\n",
    "                r=requests.get(url)\n",
    "#        print(r)\n",
    "                soup=bs(r.content,'html.parser')\n",
    "                try:\n",
    "                    job_desc_list_all.append(soup.find(\"div\",{\"class\":\"description__text description__text--rich\"}).get_text(strip=True))\n",
    "                except:\n",
    "                    job_desc_list_all.append('NA')\n",
    "                url_list_all.append(url)\n",
    "        \n",
    " \n",
    "        jobs_df = pd.DataFrame(\n",
    "            {\n",
    "            \"position\" : job_list_all,\n",
    "            \"company\" : company_list_all,\n",
    "            \"location\" : location_list_all,\n",
    "            \"job_id\":job_id_list_all,\n",
    "            \"job_description\":job_desc_list_all,\n",
    "            \"url\":url_list_all\n",
    "            }\n",
    "        )\n",
    "    \n",
    "   # spinner.ok(\"âœ”\")\n",
    "    return(jobs_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8b04b6-b4b4-4e72-8b3e-c0648eb4204a",
   "metadata": {},
   "source": [
    "## Let's try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "613a4074-f1a5-4657-b552-a5f884f7a7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             \r"
     ]
    }
   ],
   "source": [
    "jobs_df=get_linkedin_job_listing(\"data analyst\",\"hamburg\",100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "82f0ed20-aea8-4887-a1f4-e4dab58653be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7c56666b-fa3a-41a9-befe-c3f43546ccc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Lenovo'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5ab87dfc-dfeb-463d-846a-88863b0f70ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_df.to_csv(\"data_analyst_hamburg_100.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eabb6d3f-0db6-4e70-97ff-7be1887b1375",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'jobs_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mjobs_df\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'jobs_df' is not defined"
     ]
    }
   ],
   "source": [
    "jobs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55473f85-e402-4504-a550-85384616b273",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
